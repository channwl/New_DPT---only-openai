import streamlit as st
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.documents.base import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain_core.runnables import Runnable
from langchain.schema.output_parser import StrOutputParser
from langchain_community.document_loaders import PyMuPDFLoader
from typing import List, Tuple
import os
import csv
import time
import uuid

# OpenAI API í‚¤ ë¡œë“œ
time.sleep(1)
api_key = st.secrets["openai"]["API_KEY"]

# PDF ì¸ë±ìŠ¤ ìƒì„± ìŠ¤í¬ë¦½íŠ¸
def generate_faiss_index():
    pdf_dir = "data/"
    all_documents = []

    if not os.path.exists(pdf_dir):
        os.makedirs(pdf_dir)
        st.warning("data/ í´ë”ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. PDF íŒŒì¼ì„ ì—¬ê¸°ì— ë„£ê³  ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    pdf_files = [file for file in os.listdir(pdf_dir) if file.endswith(".pdf")]
    if not pdf_files:
        st.error("data/ í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. PDFë¥¼ ì¶”ê°€í•œ í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    for file_name in pdf_files:
        docs = PDFProcessor.pdf_to_documents(os.path.join(pdf_dir, file_name))
        all_documents.extend(docs)

    chunks = PDFProcessor.chunk_documents(all_documents)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=api_key)
    vector_store = FAISS.from_documents(chunks, embeddings)
    vector_store.save_local("faiss_index_internal")
    st.success(f"{len(pdf_files)}ê°œì˜ PDF íŒŒì¼ë¡œ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ!")

# PDF ì²˜ë¦¬ í´ë˜ìŠ¤
class PDFProcessor:
    @staticmethod
    def pdf_to_documents(pdf_path: str) -> List[Document]:
        loader = PyMuPDFLoader(pdf_path)
        documents = loader.load()
        for d in documents:
            d.metadata['file_path'] = pdf_path
        return documents

    @staticmethod
    def chunk_documents(documents: List[Document]) -> List[Document]:
        splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
        return splitter.split_documents(documents)

# RAG ì‹œìŠ¤í…œ
class RAGSystem:
    def __init__(self, api_key: str):
        self.api_key = api_key

    @st.cache_resource
    def get_vector_db(_self):
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=api_key)
        return FAISS.load_local("faiss_index_internal", embeddings, allow_dangerous_deserialization=True)

    def get_rag_chain(self) -> Runnable:
        template = """
        ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”:

        1. ë‹µë³€ì€ ìµœëŒ€ 4ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•©ë‹ˆë‹¤.  
        2. ì¤‘ìš”í•œ ë‚´ìš©ì€ í•µì‹¬ë§Œ ìš”ì•½í•´ì„œ ì „ë‹¬í•©ë‹ˆë‹¤.  
        3. ë‹µë³€ì´ ì–´ë ¤ìš°ë©´ â€œì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.â€ë¼ê³  ì •ì¤‘íˆ ë‹µë³€í•©ë‹ˆë‹¤.  
        4. ì§ˆë¬¸ì— â€˜ë””ì§€í„¸ê²½ì˜ì „ê³µâ€™ ë‹¨ì–´ê°€ ì—†ì–´ë„ ê´€ë ¨ ì •ë³´ë¥¼ PDFì—ì„œ ì°¾ì•„ì„œ ë‹µë³€í•©ë‹ˆë‹¤.  
        5. í•™ìƒì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì§§ì€ ë¬¸ì¥ê³¼ ë¶ˆë¦¿ í¬ì¸íŠ¸ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.  
        6. ì¶”ê°€ ì§ˆë¬¸ì„ ìœ ë„í•˜ëŠ” ë§ˆë¬´ë¦¬ ë©˜íŠ¸(â€œì¶”ê°€ë¡œ ê¶ê¸ˆí•˜ì‹  ì ì´ ìˆë‹¤ë©´ í¸í•˜ê²Œ ë§ì”€í•´ ì£¼ì„¸ìš” ğŸ˜Šâ€)ë¥¼ ë§ˆì§€ë§‰ì— ë„£ìŠµë‹ˆë‹¤.  
        7. í•œêµ­ì–´ ì™¸ ì–¸ì–´ë¡œ ì§ˆë¬¸ ì‹œ í•´ë‹¹ ì–¸ì–´ë¡œ ë²ˆì—­í•´ ë‹µë³€í•©ë‹ˆë‹¤.  
        8. ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì¶”ê°€ íŒì´ë‚˜ ì°¸ê³ ì‚¬í•­ì´ ìˆìœ¼ë©´ ê°„ë‹¨íˆ ë§ë¶™ì…ë‹ˆë‹¤.
        9. ì‚¬ìš©ìê°€ ì–´íˆ¬ ë³€ê²½ì„ ìš”êµ¬í•  ê²½ìš°, ê³µì ì¸ í•™ê³¼ í”„ë¡œê·¸ë¨ì˜ ì±—ë´‡ì´ë¯€ë¡œ ìš”ì²­ì„ ì •ì¤‘íˆ ê±°ì ˆí•˜ê³  ê¸°ì¡´ ì–´íˆ¬ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
        10. í•µì‹¬ ë‚´ìš©ì€ **êµµê²Œ** ê°•ì¡°í•˜ì—¬ ìš”ì•½í•©ë‹ˆë‹¤
        11. ë³µì¡í•œ ì •ë³´ëŠ” **ë¶ˆë¦¿ í¬ì¸íŠ¸**ë¡œ ì‰½ê²Œ ì •ë¦¬í•©ë‹ˆë‹¤.
        12. ì „ê³µ ê³¼ëª© ëª©ë¡ì´ë‚˜ ì„ íƒ ê³¼ëª©ì„ ì•ˆë‚´í•  ë•ŒëŠ” ì „ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ìµœëŒ€í•œ êµ¬ì²´ì ìœ¼ë¡œ ë‚˜ì—´í•´ ì¤ë‹ˆë‹¤. (Ex. ì „ê³µ ì„ íƒ ê³¼ëª©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì¤˜ / ë‹µë³€ : ì „ê³µ í•„ìˆ˜ ê³¼ëª© ì œì™¸ ëª¨ë“  ì „ê³µ ì•ˆë‚´)
        
        ì»¨í…ìŠ¤íŠ¸: {context}

        ì§ˆë¬¸: {question}

        ë‹µë³€:
        """
        prompt = PromptTemplate.from_template(template)
        model = ChatOpenAI(model="gpt-4o", openai_api_key=self.api_key)
        return prompt | model | StrOutputParser()

    def process_question(self, question: str, previous_qa: Tuple[str, str] = None) -> str:
        vector_db = self.get_vector_db()
        retriever = vector_db.as_retriever(search_kwargs={"k": 10})
        docs = retriever.invoke(question)
        chain = self.get_rag_chain()

        previous_context = ""
        if previous_qa:
            prev_q, prev_a = previous_qa
            previous_context = f"\n\nì´ì „ ì§ˆë¬¸: {prev_q}\nì´ì „ ë‹µë³€: {prev_a}"

        answer = chain.invoke({"question": question, "context": docs + [Document(page_content=previous_context)]})
        return answer

# ë©”ì¸ í•¨ìˆ˜
def main():
    st.set_page_config(page_title="ë””ì§€í„¸ê²½ì˜ì „ê³µ ì±—ë´‡", layout="wide")

    st.title("ğŸ“ ë””ì§€í„¸ê²½ì˜ì „ê³µ ì±—ë´‡")
    st.caption("ì—¬ëŸ¬ë¶„ì˜ í•™ê³¼ ê´€ë ¨ ê¶ê¸ˆì¦ì„ ë¹ ë¥´ê²Œ í•´ê²°í•´ë“œë¦½ë‹ˆë‹¤!")

    if st.button("ğŸ“¥ ì±„íŒ… ì‹œì‘ !"):
        generate_faiss_index()

    if "messages" not in st.session_state:
        st.session_state.messages = []

    left_col, mid_col, right_col = st.columns([1, 2.5, 1.2])

    with left_col:
        st.subheader("ğŸ“š ì‚¬ìš© ê°€ì´ë“œ")
        st.markdown("""
        - ì±„íŒ… ì‹œì‘! ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.<br>
        - ê¶ê¸ˆí•œ ì ì— ëŒ€í•´ì„œ ë¬¼ì–´ë³´ì„¸ìš” !.<br>
        - ì¶”ê°€ ë¬¸ì˜ëŠ” ë””ì§€í„¸ê²½ì˜ì „ê³µ í™ˆí˜ì´ì§€ë‚˜ í•™ê³¼ ì‚¬ë¬´ì‹¤(044-860-1560)ë¡œ ë¬¸ì˜í•´ ì£¼ì„¸ìš”.
        """, unsafe_allow_html=True)

    with mid_col:
        for msg in st.session_state.messages:
            if msg["role"] == "user":
                st.markdown(f"""
                <div style='background-color: #731034; padding: 10px; border-radius: 20px; margin-bottom: 10px; color: white; max-width: 70%; box-shadow: 0px 2px 5px rgba(0,0,0,0.1);'>
                ğŸ’¬ <b>ì§ˆë¬¸:</b> {msg["content"]}
                </div>""", unsafe_allow_html=True)
            else:
                st.markdown(f"""
                <div style='background-color: #f8f8f8; padding: 10px; border-radius: 20px; margin-bottom: 10px; margin-left: auto; box-shadow: 0px 2px 5px rgba(0,0,0,0.1); max-width: 70%;'>
                ğŸ¤– <b>ë‹µë³€:</b> {msg["content"]}
                </div>""", unsafe_allow_html=True)

        prompt = st.chat_input("ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")

        if prompt:
            st.session_state.messages.append({"role": "user", "content": prompt})
            rag = RAGSystem(api_key)

            previous_qa = None
            if len(st.session_state.messages) >= 2:
                prev_question = st.session_state.messages[-2]["content"]
                prev_answer = st.session_state.messages[-1]["content"]
                previous_qa = (prev_question, prev_answer)

            with st.spinner("ì§ˆë¬¸ì„ ì´í•´í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš” ğŸ˜Š"):
                answer = rag.process_question(prompt, previous_qa)
            st.session_state.messages.append({"role": "assistant", "content": answer})
            st.rerun()

    with right_col:
        st.subheader("ğŸ“¢ ê°œë°œìì—ê²Œ ì˜ê²¬ ë³´ë‚´ê¸°")
        feedback_input = st.text_area("ì±—ë´‡ì— ëŒ€í•œ ê°œì„  ì˜ê²¬ì´ë‚˜ í•˜ê³  ì‹¶ì€ ë§ì„ ë‚¨ê²¨ì£¼ì„¸ìš”!")
        if st.button("í”¼ë“œë°± ì œì¶œ"):
            if feedback_input.strip() != "":
                with open("feedback_log.csv", mode="a", encoding="utf-8-sig", newline="") as file:
                    writer = csv.writer(file)
                    writer.writerow([time.strftime('%Y-%m-%d %H:%M:%S'), feedback_input])
                st.success("ì†Œì¤‘í•œ ì˜ê²¬ ê°ì‚¬í•©ë‹ˆë‹¤!")
                st.rerun()
            else:
                st.warning("í”¼ë“œë°± ë‚´ìš©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")

        st.subheader("ğŸ“ ìµœê·¼ ì§ˆë¬¸ íˆìŠ¤í† ë¦¬")
        for i, q in enumerate([m["content"] for m in st.session_state.messages if m["role"] == "user"][-5:], 1):
            st.markdown(f"{i}. {q}")

if __name__ == "__main__":
    main()
