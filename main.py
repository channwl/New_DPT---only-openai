import streamlit as st
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.documents.base import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain_core.runnables import Runnable
from langchain.schema.output_parser import StrOutputParser
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.memory import ConversationSummaryMemory
from typing import List, Tuple
import os
import csv
import time

# LangSmith ê´€ë ¨ íŒ¨í‚¤ì§€ ì¶”ê°€
from langsmith import Client
import os
from langchain.callbacks.tracers.langchain import LangChainTracer
from langchain.callbacks.manager import CallbackManager
from langchain.smith import RunEvalConfig
from langchain.callbacks.tracers import LangChainTracer
from langsmith.evaluation import EvaluationResult

# API í‚¤ ë¡œë“œ
time.sleep(1)
api_key = st.secrets["openai"]["API_KEY"]

# LangSmith API í‚¤ ë° í”„ë¡œì íŠ¸ ì„¤ì •
os.environ["LANGCHAIN_TRACING_V2"] = "true"  # LangSmith ì¶”ì  í™œì„±í™”
os.environ["LANGCHAIN_API_KEY"] = st.secrets["langsmith"]["API_KEY"]  # LangSmith API í‚¤
os.environ["LANGCHAIN_PROJECT"] = "ë””ì§€í„¸ê²½ì˜ì „ê³µ_ì±—ë´‡"  # í”„ë¡œì íŠ¸ ì´ë¦„ ì„¤ì •

# PDF ì²˜ë¦¬ í´ë˜ìŠ¤ ì •ì˜ (ë³€ê²½ ì—†ìŒ)
class PDFProcessor:
    #PDFë¥¼ ë¬¸ì„œ listë¡œ ë³€í™˜
    @staticmethod
    def pdf_to_documents(pdf_path: str) -> List[Document]:
        loader = PyMuPDFLoader(pdf_path)
        documents = loader.load()
        for d in documents:
            d.metadata['file_path'] = pdf_path
        return documents
    #chunking!
    @staticmethod
    def chunk_documents(documents: List[Document]) -> List[Document]:
        splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
        return splitter.split_documents(documents)

# PDF ì¸ë±ìŠ¤ ìƒì„± í•¨ìˆ˜ (ë³€ê²½ ì—†ìŒ)
def generate_faiss_index():
    pdf_dir = "data/"
    all_documents = []
    
    if not os.path.exists(pdf_dir):
        os.makedirs(pdf_dir)
        st.warning("data/ í´ë”ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. PDF íŒŒì¼ì„ ì—¬ê¸°ì— ë„£ê³  ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return

    pdf_files = [file for file in os.listdir(pdf_dir) if file.endswith(".pdf")]
    if not pdf_files:
        st.error("data/ í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. PDFë¥¼ ì¶”ê°€í•œ í›„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return

    #PDF íŒŒì¼ ë¬¸ì„œí™”
    for file_name in pdf_files:
        docs = PDFProcessor.pdf_to_documents(os.path.join(pdf_dir, file_name))
        all_documents.extend(docs)

    #ë¬¸ì„œ chunking, vector embedding ìƒì„±, ì¸ë±ì‹±
    chunks = PDFProcessor.chunk_documents(all_documents)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=api_key)
    vector_store = FAISS.from_documents(chunks, embeddings)
    vector_store.save_local("faiss_index_internal")
    st.success(f"{len(pdf_files)}ê°œì˜ PDF íŒŒì¼ë¡œ ì¸ë±ìŠ¤ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

#RAG (LangSmith í†µí•©)
class RAGSystem:
    def __init__(self, api_key: str):
        self.api_key = api_key
        
        # LangSmith íŠ¸ë ˆì´ì„œ ì„¤ì •
        self.tracer = LangChainTracer(project_name=os.environ["LANGCHAIN_PROJECT"])
        self.callback_manager = CallbackManager([self.tracer])

        # LLM ì´ˆê¸°í™” (ì½œë°± ë§¤ë‹ˆì € ì¶”ê°€)
        self.llm = ChatOpenAI(
            model="gpt-4o", 
            openai_api_key=self.api_key, 
            temperature=0,
            callback_manager=self.callback_manager
        )
        
        # ëŒ€í™” ìš”ì•½ ë©”ëª¨ë¦¬ ì¶”ê°€
        self.memory = ConversationSummaryMemory(
            llm=self.llm, 
            return_messages=True,
            max_token_limit=300,
            memory_key="history",
            input_key="input",
            output_key="output"
        )

        # RAG chain êµ¬ì„±
        self.rag_chain = self.get_rag_chain()
        
        # LangSmith í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.langsmith_client = Client()

    # vector DB ë¶ˆëŸ¬ì˜¤ê¸° (ë³€ê²½ ì—†ìŒ)
    @st.cache_resource
    def get_vector_db(_self):
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=_self.api_key)
        return FAISS.load_local("faiss_index_internal", embeddings, allow_dangerous_deserialization=True)

    # prompt template êµ¬ì„± + RAG chain êµ¬ì„± (íŠ¸ë ˆì´ì„œ ì¶”ê°€)
    def get_rag_chain(self) -> Runnable:
        template = """
        ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ì™€ ëŒ€í™” ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”:

        1. ë‹µë³€ì€ ìµœëŒ€ 4ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•©ë‹ˆë‹¤.
        2. ì¤‘ìš”í•œ ë‚´ìš©ì€ í•µì‹¬ë§Œ ìš”ì•½í•´ì„œ ì „ë‹¬í•©ë‹ˆë‹¤.
        3. ë‹µë³€ì´ ì–´ë ¤ìš°ë©´ "ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤."ë¼ê³  ì •ì¤‘íˆ ë‹µë³€í•©ë‹ˆë‹¤.
        4. ì§ˆë¬¸ì— 'ë””ì§€í„¸ê²½ì˜ì „ê³µ' ë‹¨ì–´ê°€ ì—†ì–´ë„ ê´€ë ¨ ì •ë³´ë¥¼ PDFì—ì„œ ì°¾ì•„ì„œ ë‹µë³€í•©ë‹ˆë‹¤.
        5. ì´í•´í•˜ê¸° ì‰¬ìš´ ì§§ì€ ë¬¸ì¥ê³¼ ë¶ˆë¦¿ í¬ì¸íŠ¸ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.
        6. ë§ˆì§€ë§‰ì— "ì¶”ê°€ë¡œ ê¶ê¸ˆí•˜ì‹  ì ì´ ìˆë‹¤ë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ì£¼ì„¸ìš”."ë¼ê³  ì•ˆë‚´í•©ë‹ˆë‹¤.
        7. í•œêµ­ì–´ ì™¸ ì–¸ì–´ë¡œ ì§ˆë¬¸ ì‹œ í•´ë‹¹ ì–¸ì–´ë¡œ ë²ˆì—­í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤.
        8. ê´€ë ¨ëœ ì°¸ê³  ì‚¬í•­ì´ ìˆë‹¤ë©´ ê°„ë‹¨íˆ ë§ë¶™ì…ë‹ˆë‹¤.
        9. ì±—ë´‡ ì–´íˆ¬ëŠ” í•­ìƒ ì¹œì ˆí•˜ê³  ë‹¨ì •í•˜ê²Œ ìœ ì§€í•©ë‹ˆë‹¤.
        10. í•µì‹¬ ë‚´ìš©ì€ **êµµê²Œ** í‘œì‹œí•´ ê°•ì¡°í•©ë‹ˆë‹¤.
        11. ë³µì¡í•œ ì •ë³´ëŠ” **ë¶ˆë¦¿ í¬ì¸íŠ¸**ë¡œ ìš”ì•½ ì •ë¦¬í•©ë‹ˆë‹¤.
        12. ì „ê³µ ê³¼ëª© ì•ˆë‚´ ì‹œì—ëŠ” ì „ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ë‚˜ì—´í•©ë‹ˆë‹¤.
        13. ì¶”ê°€ ì•ˆë‚´ëŠ” "ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ì ì´ ìˆë‹¤ë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ì£¼ì„¸ìš”."ë¡œ ë§ˆë¬´ë¦¬í•©ë‹ˆë‹¤.
        14. ê°™ì€ ë§ì„ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”

        ì´ì „ ëŒ€í™” ìš”ì•½: {history}
        
        ì»¨í…ìŠ¤íŠ¸: {context}
        ì§ˆë¬¸: {question}

        ë‹µë³€:
        """
        prompt = PromptTemplate.from_template(template)
        chain = prompt | self.llm | StrOutputParser()
        
        # ì²´ì¸ì— íƒœê·¸ ì¶”ê°€ (LangSmithì—ì„œ ì¡°íšŒí•  ë•Œ ìœ ìš©)
        chain.with_config(tags=["ë””ì§€í„¸ê²½ì˜ì „ê³µ_ì±—ë´‡", "í•™ê³¼_ì •ë³´"])
        
        return chain

    # ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê³  ë‹µë³€ ë°˜í™˜ (LangSmith ì¶”ì  ê¸°ëŠ¥ ì¶”ê°€)
    def process_question(self, question: str) -> str:
        # LangSmith Run ì¶”ì  ì‹œì‘
        with self.tracer.capture_run(run_type="chain", name="ë””ì§€í„¸ê²½ì˜ì „ê³µ_ì±—ë´‡_ì§ˆì˜ì‘ë‹µ") as run:
            # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            vector_db = self.get_vector_db()
            retriever = vector_db.as_retriever(search_kwargs={"k": 7})
            docs = retriever.invoke(question)

            # ëŒ€í™” ê¸°ë¡ ìš”ì•½ ë¶ˆëŸ¬ì˜¤ê¸°
            history_summary = self.memory.load_memory_variables({})['history']

            # LLM í˜¸ì¶œ
            answer = self.rag_chain.invoke({
                "question": question,
                "context": docs,
                "history": history_summary,
            })

            # ëŒ€í™” ë‚´ìš©ì„ memoryì— ì €ì¥
            self.memory.save_context({"input": question}, {"output": answer})
            
            # LangSmithì— ê²°ê³¼ ê¸°ë¡
            run.end(outputs={"answer": answer})
            
            # ê°„ë‹¨í•œ í‰ê°€ ì‹¤í–‰ (ì„ íƒì‚¬í•­)
            self.evaluate_response(question, answer)
            
            return answer
            
    # ì‘ë‹µ í‰ê°€ ë©”ì„œë“œ (LangSmith í‰ê°€ ê¸°ëŠ¥)
    def evaluate_response(self, question: str, answer: str):
        try:
            # ê°„ë‹¨í•œ í‰ê°€ ì‹¤í–‰ - ë‹µë³€ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” í”„ë¡¬í”„íŠ¸
            eval_chain = ChatOpenAI(
                model="gpt-4o",
                openai_api_key=self.api_key,
                temperature=0
            ) | StrOutputParser()
            
            eval_prompt = f"""
            ë‹¤ìŒ ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ í’ˆì§ˆì„ 1-10ì  ì‚¬ì´ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”:
            
            ì§ˆë¬¸: {question}
            ë‹µë³€: {answer}
            
            ë‹µë³€ì´ ì§ˆë¬¸ì— ì–¼ë§ˆë‚˜ ì˜ ëŒ€ë‹µí–ˆëŠ”ì§€, ë‹µë³€ì´ ì–¼ë§ˆë‚˜ ëª…í™•í•˜ê³  ì •í™•í•œì§€ í‰ê°€í•´ì£¼ì„¸ìš”.
            ì ìˆ˜ë§Œ ìˆ«ìë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”.
            """
            
            # í‰ê°€ ì‹¤í–‰
            score = eval_chain.invoke(eval_prompt)
            
            # LangSmithì— í‰ê°€ ê²°ê³¼ ì €ì¥
            self.langsmith_client.create_evaluation(
                evaluation_name="ë‹µë³€_í’ˆì§ˆ_ì ìˆ˜",
                value=float(score) if score.strip().isdigit() else 5.0,  # ê¸°ë³¸ê°’ 5ì 
                evaluation_type="qa_score",
                source={
                    "question": question,
                    "answer": answer
                },
                target_run_id=self.tracer.run_id  # í˜„ì¬ ì¶”ì  ì¤‘ì¸ ì‹¤í–‰ì˜ ID
            )
        except Exception as e:
            # í‰ê°€ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ìˆì–´ë„ ì‚¬ìš©ì ê²½í—˜ì— ì˜í–¥ì„ ì£¼ì§€ ì•Šë„ë¡ í•¨
            print(f"í‰ê°€ ì˜¤ë¥˜: {e}")
            pass

# ë©”ì¸ í•¨ìˆ˜ (ëŒ€ë¶€ë¶„ ë³€ê²½ ì—†ìŒ)
def main():
    st.set_page_config(page_title="ë””ì§€í„¸ê²½ì˜ì „ê³µ ì±—ë´‡", layout="wide")
    st.title("ğŸ“ ë””ì§€í„¸ê²½ì˜ì „ê³µ ì±—ë´‡")
    st.caption("í•™ê³¼ì— ëŒ€í•œ ë‹¤ì–‘í•œ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ë“œë¦½ë‹ˆë‹¤.")

    # LangSmith ëŒ€ì‹œë³´ë“œ ë§í¬ ì¶”ê°€
    if st.sidebar.checkbox("ê°œë°œ ëª¨ë“œ ë³´ê¸°"):
        st.sidebar.markdown("### LangSmith ëŒ€ì‹œë³´ë“œ")
        st.sidebar.markdown("[ëŒ€ì‹œë³´ë“œ ì—´ê¸°](https://smith.langchain.com)")
        st.sidebar.info("LangSmith ëŒ€ì‹œë³´ë“œì—ì„œ ì±—ë´‡ì˜ ë™ì‘ê³¼ ì„±ëŠ¥ì„ ëª¨ë‹ˆí„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    #ì´ ë²„íŠ¼ í´ë¦­ ì‹œ PDF ì¸ë±ìŠ¤ ìƒì„±
    if st.button("ğŸ“¥ ì±„íŒ… ì‹œì‘ !"):
        generate_faiss_index()

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ëŒ€í™” ë¡œê·¸ ì €ì¥)
    if "messages" not in st.session_state:
        st.session_state.messages = []

    #í˜ì´ì§€ 3ë‹¨êµ¬ì„±
    left_col, mid_col, right_col = st.columns([1, 2.5, 1.2])

    #left : ì‚¬ìš© ê°€ì´ë“œ
    with left_col:
        st.subheader("ğŸ“š ì‚¬ìš© ê°€ì´ë“œ")
        st.markdown("""
        - 'ğŸ“¥ ì±„íŒ… ì‹œì‘ !' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.<br>
        - ê¶ê¸ˆí•œ ë‚´ìš©ì„ ì…ë ¥í•˜ì‹œë©´ ê´€ë ¨ ì •ë³´ë¥¼ PDF ê¸°ë°˜ìœ¼ë¡œ ì•ˆë‚´í•´ë“œë¦½ë‹ˆë‹¤.<br>
        - ì¶”ê°€ ë¬¸ì˜ëŠ” ë””ì§€í„¸ê²½ì˜ì „ê³µ í™ˆí˜ì´ì§€ ë˜ëŠ” í•™ê³¼ ì‚¬ë¬´ì‹¤(044-860-1560)ë¡œ ì—°ë½ ë°”ëë‹ˆë‹¤.
        """, unsafe_allow_html=True)

    #mid : ì±„íŒ… ê¸°ë¡ í‘œì‹œ ë° ì…ë ¥
    with mid_col:
        for msg in st.session_state.messages:
            if msg["role"] == "user":
                st.markdown(f"""
                <div style='background-color: #731034; padding: 10px; border-radius: 20px; margin-bottom: 10px; color: white; max-width: 70%; box-shadow: 0px 2px 5px rgba(0,0,0,0.1);'>
                ğŸ’¬ <b>ì§ˆë¬¸:</b> {msg["content"]}
                </div>""", unsafe_allow_html=True)
            else:
                st.markdown(f"""
                <div style='background-color: #f8f8f8; padding: 10px; border-radius: 20px; margin-bottom: 10px; margin-left: auto; box-shadow: 0px 2px 5px rgba(0,0,0,0.1); max-width: 70%;'>
                ğŸ¤– <b>ë‹µë³€:</b> {msg["content"]}
                </div>""", unsafe_allow_html=True)

        #ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥ ë° ì²˜ë¦¬
        prompt = st.chat_input("ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        if prompt:
            st.session_state.messages.append({"role": "user", "content": prompt})
            rag = RAGSystem(st.secrets["openai"]["API_KEY"])

            with st.spinner("ì§ˆë¬¸ì„ ì´í•´í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
                answer = rag.process_question(prompt)

            st.session_state.messages.append({"role": "assistant", "content": answer})
            st.rerun()

    # right : í”¼ë“œë°± ë° ìµœê·¼ ì§ˆë¬¸
    with right_col:
        st.subheader("ğŸ“¢ ê°œë°œìì—ê²Œ ì˜ê²¬ ë³´ë‚´ê¸°")
        feedback_input = st.text_area("ì±—ë´‡ì— ëŒ€í•œ ê°œì„  ì˜ê²¬ì´ë‚˜ í•˜ê³  ì‹¶ì€ ë§ì„ ë‚¨ê²¨ì£¼ì„¸ìš”.")
        if st.button("í”¼ë“œë°± ì œì¶œ"):
            if feedback_input.strip() != "":
                with open("feedback_log.csv", mode="a", encoding="utf-8-sig", newline="") as file:
                    writer = csv.writer(file)
                    writer.writerow([time.strftime('%Y-%m-%d %H:%M:%S'), feedback_input])
                
                # LangSmithì— í”¼ë“œë°± ì €ì¥ (ì„ íƒì‚¬í•­)
                if "rag" in locals():
                    try:
                        rag.langsmith_client.create_dataset(
                            dataset_name="ì‚¬ìš©ì_í”¼ë“œë°±",
                            description="ì‚¬ìš©ìë¡œë¶€í„° ë°›ì€ í”¼ë“œë°±"
                        )
                        rag.langsmith_client.create_example(
                            dataset_name="ì‚¬ìš©ì_í”¼ë“œë°±",
                            inputs={},
                            outputs={"feedback": feedback_input, "timestamp": time.strftime('%Y-%m-%d %H:%M:%S')}
                        )
                    except:
                        pass  # ì´ë¯¸ ë°ì´í„°ì…‹ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš° ì—ëŸ¬ ë°©ì§€
                
                st.success("ì†Œì¤‘í•œ ì˜ê²¬ ê°ì‚¬í•©ë‹ˆë‹¤.")
                st.rerun()
            else:
                st.warning("í”¼ë“œë°± ë‚´ìš©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")

        st.subheader("ğŸ“ ìµœê·¼ ì§ˆë¬¸ íˆìŠ¤í† ë¦¬")
        for i, q in enumerate([m["content"] for m in st.session_state.messages if m["role"] == "user"][-5:], 1):
            st.markdown(f"{i}. {q}")

#streamlit ì•± ì‹¤í–‰ ì‹œì‘
if __name__ == "__main__":
    main()
