import streamlit as st
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.documents.base import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain_core.runnables import Runnable
from langchain.schema.output_parser import StrOutputParser
from langchain_community.document_loaders import PyMuPDFLoader
from typing import List
import os
import csv
import time

# OpenAI API í‚¤ ë¡œë“œ
time.sleep(1)
api_key = st.secrets["openai"]["API_KEY"]

# PDF ì¸ë±ìŠ¤ ìƒì„± í•¨ìˆ˜
def generate_faiss_index():
    pdf_dir = "data/"
    all_documents = []

    if not os.path.exists(pdf_dir):
        os.makedirs(pdf_dir)
        st.warning("data/ í´ë”ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. PDF íŒŒì¼ì„ ë„£ê³  ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    pdf_files = [file for file in os.listdir(pdf_dir) if file.endswith(".pdf")]
    if not pdf_files:
        st.error("data/ í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. PDFë¥¼ ì¶”ê°€í•œ í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    for file_name in pdf_files:
        docs = PDFProcessor.pdf_to_documents(os.path.join(pdf_dir, file_name))
        all_documents.extend(docs)

    chunks = PDFProcessor.chunk_documents(all_documents)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=api_key)
    vector_store = FAISS.from_documents(chunks, embeddings)
    vector_store.save_local("faiss_index_internal")
    st.success(f"{len(pdf_files)}ê°œì˜ PDF íŒŒì¼ë¡œ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ!")

# PDF ì²˜ë¦¬ í´ë˜ìŠ¤
class PDFProcessor:
    @staticmethod
    def pdf_to_documents(pdf_path: str) -> List[Document]:
        loader = PyMuPDFLoader(pdf_path)
        documents = loader.load()
        for d in documents:
            d.metadata['file_path'] = os.path.basename(pdf_path)
        return documents

    @staticmethod
    def chunk_documents(documents: List[Document]) -> List[Document]:
        splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
        return splitter.split_documents(documents)

# RAG ì‹œìŠ¤í…œ with memory
class RAGSystem:
    def __init__(self, api_key: str):
        self.api_key = api_key

    @st.cache_resource
    def get_vector_db(_self):
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=api_key)
        return FAISS.load_local("faiss_index_internal", embeddings, allow_dangerous_deserialization=True)

    def get_rag_chain(self, chat_history: str) -> Runnable:
        template = """
        ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”:

        ì´ì „ ëŒ€í™” ë‚´ìš©:
        {chat_history}

        1. ë‹µë³€ì€ ìµœëŒ€ 4ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•©ë‹ˆë‹¤.  
        2. ì¤‘ìš”í•œ ë‚´ìš©ì€ í•µì‹¬ë§Œ ìš”ì•½í•´ì„œ ì „ë‹¬í•©ë‹ˆë‹¤.  
        3. ë‹µë³€ì´ ì–´ë ¤ìš°ë©´ â€œì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.â€ë¼ê³  ì •ì¤‘íˆ ë‹µë³€í•©ë‹ˆë‹¤.  
        4. ì§ˆë¬¸ì— â€˜ë””ì§€í„¸ê²½ì˜ì „ê³µâ€™ ë‹¨ì–´ê°€ ì—†ì–´ë„ ê´€ë ¨ ì •ë³´ë¥¼ PDFì—ì„œ ì°¾ì•„ì„œ ë‹µë³€í•©ë‹ˆë‹¤.  
        5. í•™ìƒì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì§§ì€ ë¬¸ì¥ê³¼ ë¶ˆë¦¿ í¬ì¸íŠ¸ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.  
        6. ë‹µë³€ ì‹œì‘ ì‹œ, ì´ì „ ì§ˆë¬¸ê³¼ ì´ì–´ì§€ëŠ” ì§ˆë¬¸ì¼ ê²½ìš° "ì•ì„œ ì•Œë ¤ë“œë¦° ë‚´ìš©ì— ì´ì–´ì„œ," ë¼ëŠ” ë©˜íŠ¸ë¡œ ì‹œì‘í•˜ì„¸ìš”.  
        7. ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” í•­ìƒ â€œì¶”ê°€ë¡œ ê¶ê¸ˆí•˜ì‹  ì ì´ ìˆë‹¤ë©´ í¸í•˜ê²Œ ë§ì”€í•´ ì£¼ì„¸ìš” ğŸ˜Šâ€ë¼ëŠ” ë©˜íŠ¸ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.  
        8. ìˆ«ì, ê¸°ê°„ ë“± ì •ë³´ëŠ” ë³´ê¸° ì‰½ë„ë¡ **êµµê²Œ** í‘œì‹œí•©ë‹ˆë‹¤.  
        9. ê´€ë ¨ëœ PDF íŒŒì¼ëª…ì´ ìˆë‹¤ë©´ "ì°¸ê³ : [íŒŒì¼ëª…]"ìœ¼ë¡œ ë§ˆì§€ë§‰ì— í‘œì‹œí•´ ì£¼ì„¸ìš”.  
        10. ì‚¬ìš©ìê°€ ì–´íˆ¬ ë³€ê²½ì„ ìš”êµ¬í•  ê²½ìš°, ì •ì¤‘íˆ ê±°ì ˆí•˜ë©° ê¸°ì¡´ ê³µì‹ ì–´íˆ¬ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.

        ì»¨í…ìŠ¤íŠ¸: {context}

        ì§ˆë¬¸: {question}

        ë‹µë³€:
        """
        prompt = PromptTemplate.from_template(template)
        model = ChatOpenAI(model="gpt-4o", openai_api_key=self.api_key)
        return prompt | model | StrOutputParser()

    def process_question(self, question: str, chat_history: str) -> str:
        vector_db = self.get_vector_db()
        retriever = vector_db.as_retriever(search_kwargs={"k": 10})
        docs = retriever.invoke(question)
        # contextì— PDF ì¶œì²˜ í‘œì‹œ
        context_with_source = "\n".join([f"ì¶œì²˜: {doc.metadata.get('file_path', 'ì•Œ ìˆ˜ ì—†ìŒ')}\n{doc.page_content}" for doc in docs])
        chain = self.get_rag_chain(chat_history)
        return chain.invoke({"question": question, "context": context_with_source, "chat_history": chat_history})
