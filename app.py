import argparse
import streamlit as st
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.documents.base import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain_core.runnables import Runnable
from langchain.schema.output_parser import StrOutputParser
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.memory import ConversationSummaryMemory
from typing import List
import os
import time

# OpenAI API í‚¤ ë¡œë“œ
time.sleep(1)
try:
    api_key = st.secrets["openai"]["API_KEY"]
except:
    api_key = os.environ.get("OPENAI_API_KEY")  # ì½˜ì†”ìš©

# PDF ì²˜ë¦¬ í´ë˜ìŠ¤
class PDFProcessor:
    @staticmethod
    def pdf_to_documents(pdf_path: str) -> List[Document]:
        loader = PyMuPDFLoader(pdf_path)
        documents = loader.load()
        for d in documents:
            d.metadata['file_path'] = pdf_path
        return documents

    @staticmethod
    def chunk_documents(documents: List[Document]) -> List[Document]:
        splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
        return splitter.split_documents(documents)

# FAISS ì¸ë±ìŠ¤ ìƒì„± í•¨ìˆ˜
def generate_faiss_index():
    pdf_dir = "data/"
    all_documents = []

    if not os.path.exists(pdf_dir):
        os.makedirs(pdf_dir)
        print("data/ í´ë”ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. PDF íŒŒì¼ì„ ì—¬ê¸°ì— ë„£ê³  ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return

    pdf_files = [file for file in os.listdir(pdf_dir) if file.endswith(".pdf")]
    if not pdf_files:
        print("data/ í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. PDFë¥¼ ì¶”ê°€í•œ í›„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return

    for file_name in pdf_files:
        docs = PDFProcessor.pdf_to_documents(os.path.join(pdf_dir, file_name))
        all_documents.extend(docs)

    chunks = PDFProcessor.chunk_documents(all_documents)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=api_key)
    vector_store = FAISS.from_documents(chunks, embeddings)
    vector_store.save_local("faiss_index_internal")
    print(f"{len(pdf_files)}ê°œì˜ PDF íŒŒì¼ë¡œ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ!")

# RAG ì‹œìŠ¤í…œ
class RAGSystem:
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.llm = ChatOpenAI(model="gpt-4o", openai_api_key=self.api_key)
        self.memory = ConversationSummaryMemory(llm=self.llm)
        self.rag_chain = self.get_rag_chain()

    def get_vector_db(self):
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=self.api_key)
        return FAISS.load_local("faiss_index_internal", embeddings, allow_dangerous_deserialization=True)

    def get_rag_chain(self) -> Runnable:
        template = """
        ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ì™€ ëŒ€í™” ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”:

        - ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ìµœëŒ€ 4ë¬¸ì¥ ì´ë‚´ë¡œ ì‘ì„±
        - í•µì‹¬ ë‚´ìš©ì€ **êµµê²Œ** í‘œì‹œ
        - ë¶ˆë¦¿ í¬ì¸íŠ¸ë¡œ ì •ë¦¬
        - ì¶”ê°€ ì§ˆë¬¸ ìœ ë„
        - ì´í•´ê°€ ì–´ë µê±°ë‚˜ ë¶ˆí™•ì‹¤í•˜ë©´ â€œì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.â€ë¼ê³  ë‹µë³€

        ëŒ€í™” ìš”ì•½: {history}
        ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸: {context}
        ì§ˆë¬¸: {question}

        ë‹µë³€:
        """
        prompt = PromptTemplate.from_template(template)
        return prompt | self.llm | StrOutputParser()

    def process_question(self, question: str) -> str:
        vector_db = self.get_vector_db()
        retriever = vector_db.as_retriever(search_kwargs={"k": 10})
        docs = retriever.invoke(question)

        history = self.memory.chat_memory.messages

        answer = self.rag_chain.invoke({
            "question": question,
            "context": docs,
            "history": history,
        })

        self.memory.save_context({"input": question}, {"output": answer})
        return answer

# ì½˜ì†” ëª¨ë“œ
def run_console_mode():
    print("ë””ì§€í„¸ê²½ì˜ì „ê³µ ì±—ë´‡ (ì½˜ì†” ëª¨ë“œ)")
    print("PDF ê¸°ë°˜ í•™ê³¼ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤. 'exit' ì…ë ¥ ì‹œ ì¢…ë£Œë©ë‹ˆë‹¤.")
    rag = RAGSystem(api_key)

    while True:
        user_input = input("ì§ˆë¬¸: ")
        if user_input.strip().lower() in {"exit", "quit"}:
            print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        answer = rag.process_question(user_input)
        print(f"ë‹µë³€: {answer}\n")

# ì›¹ì•± ë©”ì¸ (Streamlit)
def run_web_mode():
    st.set_page_config(page_title="ë””ì§€í„¸ê²½ì˜ì „ê³µ ì±—ë´‡", layout="wide")
    st.title("ğŸ“ ë””ì§€í„¸ê²½ì˜ì „ê³µ ì±—ë´‡")
    st.caption("í•™ê³¼ì— ëŒ€í•œ ë‹¤ì–‘í•œ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ë“œë¦½ë‹ˆë‹¤.")

    if st.button("ğŸ“¥ ì±„íŒ… ì‹œì‘ !"):
        generate_faiss_index()

    if "messages" not in st.session_state:
        st.session_state.messages = []

    prompt = st.chat_input("ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
    if prompt:
        st.session_state.messages.append({"role": "user", "content": prompt})
        rag = RAGSystem(api_key)

        with st.spinner("ì§ˆë¬¸ì„ ì´í•´í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
            answer = rag.process_question(prompt)

        st.session_state.messages.append({"role": "assistant", "content": answer})
        st.rerun()

    for msg in st.session_state.messages:
        role, content = msg["role"], msg["content"]
        color = "#731034" if role == "user" else "#f8f8f8"
        prefix = "ğŸ’¬ ì§ˆë¬¸" if role == "user" else "ğŸ¤– ë‹µë³€"
        st.markdown(f"""
        <div style='background-color: {color}; padding: 10px; border-radius: 15px; margin-bottom: 10px; max-width: 70%; color: {"white" if role == "user" else "black"};'>
        <b>{prefix}:</b> {content}
        </div>
        """, unsafe_allow_html=True)

# ì‹¤í–‰ ëª¨ë“œ ì„ íƒ
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--mode", type=str, choices=["console", "web"], default="web", help="ì‹¤í–‰ ëª¨ë“œ ì„ íƒ: console ë˜ëŠ” web")
    args = parser.parse_args()

    if args.mode == "console":
        run_console_mode()
    else:
        run_web_mode()
